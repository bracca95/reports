# Report 2023, May 31

This report shows the results of different feature extraction models applied to the ProtoNet pipeline. Follow the `.json` file to read the configuration used for a specific test. 

* Augmentation has been performed on breaks and marks. See Section Open Issues for more details.
* The dataset has been split as (train, val, test) = (70%, 20%, 10%)
* Cropping has been performed for square-shaped images (bubble, point, dirt)

## Models
Here the configuration used during training and test for the different feature extractor models.

### ResNet50_1
Pretrained ResNet50. 28x28 image patches were passed as input.

```json
{
    "epochs": 100,
    "crop_size": 28,
    "image_size": 28,
    "fsl": {
        "episodes": 100,
        "train_n_way": 6,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 6,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

### ResNet50_2 and HRNet_w18
Pretrained ResNet50. 224x224 (original input dimension for ResNet) image patches were passed as input. The same hols true for HRNet_w18

```json
{
    "epochs": 100,
    "crop_size": 28,
    "image_size": 224,
    "fsl": {
        "episodes": 100,
        "train_n_way": 6,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 6,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

### Default_1
This model uses the default feature extractor from ProtoNet, with the original input image size. However, only 3 classes are compared for each episode.

```json
{
    "epochs": 100,
    "crop_size": 28,
    "image_size": 105,
    "fsl": {
        "episodes": 100,
        "train_n_way": 3,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 3,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

### Default_2
This model uses the default feature extractor from ProtoNet, with the original input image size. It is trained and tested to split all the 6 classes of defects.

```json
{
    "epochs": 100,
    "crop_size": 28,
    "image_size": 105,
    "fsl": {
        "episodes": 100,
        "train_n_way": 6,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 6,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

### Default_c16
This model uses the default feature extractor from ProtoNet, with the original input image size. It is trained and tested to split all the 6 classes of defects. Bubbles, points and dirts have been cropped to 16x16 before upscaling.

```json
{
    "epochs": 100,
    "crop_size": 16,
    "image_size": 105,
    "fsl": {
        "episodes": 100,
        "train_n_way": 6,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 6,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

### Cnn_1_28
This is a simple custom model with a couple of convolutional layers, enhacned by pooling. The input size is 28x28.

```json
{
    "epochs": 100,
    "crop_size": 28,
    "image_size": 28,
    "fsl": {
        "episodes": 100,
        "train_n_way": 6,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 6,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

### Cnn_2_105
This is similar to the previous network, but the input has been upscaled to 105x105. There are two additional convolutional layers.

```json
{
    "epochs": 100,
    "crop_size": 28,
    "image_size": 105,
    "fsl": {
        "episodes": 100,
        "train_n_way": 6,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 6,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

### Cnn_s3q6
This is the equivalent of cnn_1_28 but using 3 classes as prototypes and 6 classes to learn separations (query set).

```json
{
    "epochs": 100,
    "crop_size": 28,
    "image_size": 28,
    "fsl": {
        "episodes": 100,
        "train_n_way": 3,
        "train_k_shot_s": 5,
        "train_k_shot_q": 5,
        "test_n_way": 6,
        "test_k_shot_s": 5,
        "test_k_shot_q": 5
    }
}
```

|            | bubble | point | break | dirt | mark | scratch | **OVERALL** |
| -----------|:------:|:-----:|:-----:|:----:|:----:|:-------:|:-----------:|
| ResNet50_1 | 0.8498 | 0.7780| 1.0   | 0.768|  1.0 | 0.8954  | 0.882       |
| ResNet50_2 | 0.9514 | 0.7952| 1.0   | 0.8  | 1.0  | 0.9996  | 0.924       |
| HRNet_w18  | 0.8660 | 0.7315| 1.0   | 0.686| 1.0  | 1.0     | 0.881       |
| default_1  | 0.8730 | 0.8907| 1.0   | 0.803| 1.0  | 0.9995  | 0.93        |
| default_2  | 0.7992 | 0.7214| 1.0   | 0.571| 1.0  | 1.0     | 0.848       |
| default_c16| 0.7588 | 0.6934| 1.0   | 0.659| 1.0  | 0.9994  | 0.851       |
| cnn_1_28   | 0.8210 | 0.7246| 0.9968| 0.789| 0.998| 0.937   | 0.88        |
| cnn_s3q6   | 0.7516 | 0.7462| 0.9964| 0.748| 0.997| 0.935   | 0.862       |
| cnn_2_105  | 0.8252 | 0.6118| 1.0   | 0.613| 0.998| 0.981   | 0.838       |
| MixerMLP   | 0.8144 | 0.6180| 1.0   | 0.649| 1.0  | 0.9252  | 0.834       |


## CNN comparisons
I performed a couple of comparisons between the two CNN models: in the first case the model was used as feature extractor to then feed a meta-learning pipeline (the ProtoRoutine); then, the same model was added a classification layer to perform standard supervised learning.

* cnn_1_28 gets a overall accuracy of 0.594  
* cnn_2_105 gets a overall accuracy of 0.778

## Open Issues:
Few-Shot Learning usually requires few samples, but the need of splitting the dataset into three (at least two, if evaluation is discarded) parts would eventually require that, with this configuration, I have at least 10 samples for each class in training, evaluation and test dataset. So at least 30 samples per class are required. Standard augmentation (rotations, flipping..) has been applied to classes that could not fulfil this requirement. Therefore, for those classes the accuracy is likely to be very high, because the information processed is related. Generative augmentation could represent a solution to this, unless we can collect other samples.

## References
[MixerMLP](https://huggingface.co/timm/mixer_b16_224.miil_in21k_ft_in1k)  
[ProtoNet implementation used](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch)

Protonet article:
```bib
@article{DBLP:journals/corr/SnellSZ17,
  author    = {Jake Snell and
               Kevin Swersky and
               Richard S. Zemel},
  title     = {Prototypical Networks for Few-shot Learning},
  journal   = {CoRR},
  volume    = {abs/1703.05175},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.05175},
  archivePrefix = {arXiv},
  eprint    = {1703.05175},
  timestamp = {Wed, 07 Jun 2017 14:41:38 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/SnellSZ17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
```

```bib
@article{tolstikhin2021mixer,
    title={MLP-Mixer: An all-MLP Architecture for Vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2105.01601},
  year={2021}
}
```